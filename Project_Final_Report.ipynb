{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Nutrition from Web-Scraped Recipes\n",
    "### Nema Sobhani and Naomi Goodnight\n",
    "#### Data Science Tools I: Final Project, Winter 2019"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset and Motivation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**How**  \n",
    "This data set was collected from web scraping a selection of online cooking websites and associating their ingredients with nutrition data from the USDA's Nutrient Data Library.\n",
    "\n",
    "**Why**  \n",
    "We thought that this project demonstrated some proof-of-concept potential in nutritional health and research, presented a pop-culture focus, and was both challenging and fun.\n",
    "\n",
    "**Meta Data**\n",
    "- Scraped Data\n",
    "    - This dataset started its life on various recipe database websites.  Through webscraper code, utilizing BeautifulSoup and open source code as a base, the initial csv of 155,876 lines listed the url, recipe title, total time, and each ingredient as a separate column.  Recipe titles or ingredients which internally contained commas were surrounded by double quotes.  However, as a result of the data's orgin on websites, significant data cleaning was needed.\n",
    "\n",
    "- USDA Nutrition Data \n",
    "    - These internally consistent data files contained most major food items in the American diet, along with common nutrition markers and experimental/analytic data.\n",
    "    - Typical attributes include:\n",
    "        - Food group\n",
    "        - Name\n",
    "        - Preparation\n",
    "        - Nutrient data / 100 grams (Calories, Protein, Carbs, Fat, etc.)\n",
    "        \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task Definition/Research Question"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For culinary websites looking to capture someone who shifts away from prepackaged foods with clearly-labeled nutritional content, especially with cook-it-yourself meal-delivery services growing in popularity, the need to appeal to home-cooking has created demand to provide nutritional content to consumers. \n",
    "\n",
    "To close these gaps, **we developed an automated process to return nutritional information from an input of raw web scraped recipe data**. This would allow websites to update their entire catalog of recipes to include nutritional data from the USDA and offer a significant advantage in the growing market for home cooking. \n",
    "\n",
    "This would also allow the website to conduct broad surveys of tendencies in nutritional content, ingredient frequencies, and health trends. \n",
    "\n",
    "<br>\n",
    "  \n",
    "**Future Directions**  \n",
    "As we enter our next phase, there are several areas of interest that could be incorporated into our project to both increase its accuracy and widen its scope. One focus would be to scrape more attributes, particularly location data, insights into ethnic cuisine and serving size. Incorporating machine learning techniques would offer the ability to create generic grocery lists based on a unique profile of a site. Natural language processing could be leveraged to more accurately determine the ingredients based on n-gram frequency. Visualizing popular recipes and ingredients by site, using word clouds, offers a custom representation of a website's culinary focus and style which carries innate marketing value."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Literature Review"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Fitness Trackers**  \n",
    "Mobile applications such as [\"MyFitnessPal\"](https://www.myfitnesspal.com/) are consumer level calorie-counting applications that require manual input of food items in order to retrieve nutritional data. Our application is distinct from these, since it is geared towards producers (food websites) it does not require any input from consumers. Our program would be ideal for recipe websites as it could be developed to interface with fitness apps and transfer all nutritional information to the user, removing the need for manual entry. \n",
    "\n",
    "\n",
    "**Geographic/Cultural Food Association**  \n",
    "There is a [**_Kaggle_** competition](https://www.kaggle.com/c/whats-cooking) in which a list of ingredients, provided via [**_Yummly_**](https://www.yummly.com), is used to predict the ethnicity of that dish. This is similar to some of the word matching and recognition done in  our project, but they differ in that the competition criteria  has a machine learning bent, to determine the ethnic origin of a dish using keywords. Our project focused on actual matching of the ingredients to the USDA nutrition database in order to extract nutritional content. Beyond this, the Kaggle competition used thoroughly cleaned data from only one site through an API. Ours is intended to be used with almost any recipe website and handle raw html data and return nutrition data.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quality of Cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Web Scraping\n",
    "\n",
    "**Web Scraping**\n",
    "<p>The initial webscraping origin began from <i>BS.py</i>.  This program went to the search page of several promient recipe website search pages (e.g., [Epicurious](https://www.epicurious.com/search?content=recipe)) and scraped the links of each recipe and saved the resultant links in a file.  The second stage webscraper, <i>RScrape.py</i>, utilized additional open source code, [recipe_scrapers](https://github.com/hhursev/recipe-scrapers).  The data from this stage was saved in the 150k+ csv file, <i>recipe_output.csv</i>.  Several fields included an internal comma and were thus surrounded by double quotes.  Future interations would either surround all fields with double quotes or use a unique field separator as the variability caused complexity in later phases.  An example line follows:\n",
    " - http://bbc.co.uk/food/recipes/awarmsaladofasparagu_67967, \"A warm salad of asparagus, field mushrooms and fresh peas\",30,\"20 medium asparagus spears, trimmed and peeled\",\"175g/6oz shelled young peas\",\"4 tbsp extra virgin olive oil\",\"1 clove garlic, mashed to a paste with a little salt\",\"8 small field mushrooms, stalks removed and peeled\",\"salt and ground black pepper\",\"1 ciabatta loaf, cut into two horizontally then in two from top to bottom\",\"1 shallot, finely chopped\",\"3 tbsp dry vermouth\",\"55g/2oz unsalted butter, cubed\",\"1 tbsp chopped flat-leaf parsley\",\"1 tbsp snipped chives\",\"1 tsp chopped tarragon\",\"1 tbsp freshly squeezed lemon juice\",\"30g/1oz each rocket and watercress leaves\",\n",
    "\n",
    "**Data Cleaning**\n",
    "<p>Once the web scraping was complete, the large task of cleaning the data remained.  This was done with a combination of bash scripting and python scripting (with and without pandas), _Cleaning.py_.  Several of the steps involved removal of data, on future iterations of this project more careful analysis would need to be done.  Examples of issues follow.\n",
    "\n",
    " - Lines without ingredients (likely a web scraper issue)\n",
    " - Duplicated lines (likely due to web scraper timeout and restart)\n",
    " - Lines with duplicate ingredients (website display issue)\n",
    " - Ingredient fields with instructions included\n",
    "   - \"6 medium red onions (about 7 ounces each), peeled, cut vertically into 6 wedges to within 1/2 inch of bottom\"\n",
    "   - \"4 cups any combination of fresh berries, picked over, washed, hulled, if necessary, and drained until dry, and/or cut-up peeled fresh fruit, divided (1 1/2 cups and 2 1/2 cups), (Note: try blueberries and peaches, or raspberries and nectarines, or blueberries and strawberries, or plums and peaches with Marionberries or huckleberries; do not peel plums, nectarines or pears)\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. USDA Nutrition Retrieval\n",
    "**Data Cleaning / Transformation**  \n",
    "First, raw scraped data is first split and all newlines and empty elements are removed. Second, each individual ingredient line is parsed to determine weight and quantity, using a mix of numeric parsing and keyword location for weight measurements (g, ml, tbsp, ounce, cup, etc). Third, the ingredient line is truncated to remove numerics, measurements, and stop words, which allows careful text matching in the USDA food description file. Fourth, a match is made to the USDA word description file, utilizing a heuristic to determine the ingredient similarity between both data sets.  Fifth, the USDA nutritional data file is accessed by food ID, where all data is retrieved (calories, protein, fat, carbs, etc).\n",
    "\n",
    "**Unusual Incidents**  \n",
    "There were many edge cases that come up in parsing numeric descriptors. Two, six and a half ounce steaks may be written in many ways. For example:\n",
    "- 2 6 1/2 ounce steaks\n",
    "- 2, 6½ ounce steaks\n",
    "- 2 6 ½ oz. steaks  \n",
    "\n",
    "Parsing this was tricky, and many edge cases were defined in order to handle unusual ascii unicode, as well as to parse between quantities and weights.  \n",
    "<br>\n",
    "Another error-prone issue that has come up regularly that we have not been able to fully control is mis-matching of food descriptions. We used the heuristic that states, **_\"the correct item is the one that (1) contains the highest number of word matches and (2) has the shortest character length if there is a tie.\"_**. This heuristic isn't always necessarily correct! Often there would be an incorrect match that satisfies the heuristic properties. For example, _\"1 clove of garlic\"_ may either match with _\"garlic, raw\"_ or _\"clove, spice\"_ in the USDA nutrition database, leading to a possible error.\n",
    "\n",
    "**Missing Values**  \n",
    "Due to the nature of the USDA nutritional data, if a value was not included, it means that it was not measured. These were stored in the dataframe as 'NaN' and were not included in analysis. If it was measured and it was 0, it was included.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stats and Interpretation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Web Scraping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/importlib/_bootstrap.py:219: ImportWarning: can't resolve package from __spec__ or __package__, falling back on __name__ and __path__\n",
      "  return f(*args, **kwds)\n",
      "/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/importlib/_bootstrap.py:219: ImportWarning: can't resolve package from __spec__ or __package__, falling back on __name__ and __path__\n",
      "  return f(*args, **kwds)\n",
      "/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/importlib/_bootstrap.py:219: ImportWarning: can't resolve package from __spec__ or __package__, falling back on __name__ and __path__\n",
      "  return f(*args, **kwds)\n",
      "/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/wordcloud/wordcloud.py:31: ResourceWarning: unclosed file <_io.TextIOWrapper name='/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/wordcloud/stopwords' mode='r' encoding='UTF-8'>\n",
      "  STOPWORDS = set(map(str.strip, open(os.path.join(FILE, 'stopwords')).readlines()))\n",
      "ResourceWarning: Enable tracemalloc to get the object allocation traceback\n",
      "/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/nltk/decorators.py:68: DeprecationWarning: `formatargspec` is deprecated since Python 3.5. Use `signature` and the `Signature` object directly\n",
      "  regargs, varargs, varkwargs, defaults, formatvalue=lambda value: \"\"\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(action='once')\n",
    "from NG_analysis import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As each ingredient needs to pass through the complex nutritional program, the total number of ingredients is extremely important when determining the run time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_high_level"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#The top 20 websites\n",
    "wb_topsum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15,10))\n",
    "wb_topsum.NumRecipes.sort_values(ascending=False).plot.bar()\n",
    "\n",
    "plt.xticks(rotation=50)\n",
    "plt.title('Number of Recipes per top 20 Sites')\n",
    "plt.xlabel(\"Website of Origin\")\n",
    "plt.ylabel(\"Number of Recipes\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15,10))\n",
    "wb_topsum.AveNumIngr.sort_values(ascending=False).plot.bar()\n",
    "plt.xticks(rotation=50)\n",
    "plt.title('Average Number of Ingredients per Recipe in top 20 Sites')\n",
    "plt.xlabel(\"Website of Origin\")\n",
    "plt.ylabel(\"Average Number of Ingredients\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#The top words by frequency within the recipe titles\n",
    "wordfreq_Title.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15,10))\n",
    "wordfreq_Title.head(50).sort_values(ascending=False).plot.bar()\n",
    "plt.xticks(rotation=50)\n",
    "plt.title('Word Frequency in Recipe Titles')\n",
    "plt.xlabel(\"Word\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#The same set but from the ingredients has a different distribution\n",
    "wordfreq_Ing_top"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15,10))\n",
    "wordfreq_Ing_top.sort_values(ascending=False).plot.bar()\n",
    "plt.xticks(rotation=50)\n",
    "plt.title('Word Frequency in Recipe Ingredients')\n",
    "plt.xlabel(\"Word\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are a lot of high frequency, irrelevant words. Without stopwords the list is much more interesting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wordfreq_Title_top_wo_stop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15,10))\n",
    "wordfreq_Title_top_wo_stop.sort_values(ascending=False).plot.bar()\n",
    "plt.xticks(rotation=50)\n",
    "plt.title('Word Frequency in Recipe Title (w/o StopWords)')\n",
    "plt.xlabel(\"Word\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wordfreq_AggIng_top_wo_stop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15,10))\n",
    "wordfreq_AggIng_top_wo_stop.sort_values(ascending=False).plot.bar()\n",
    "plt.xticks(rotation=50)\n",
    "plt.title('Word Frequency of Recipe Ingredients (w/o StopWords)')\n",
    "plt.xlabel(\"Word\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking through the high frequency words in the ingredient list, there are several more stopwords which could be added if one wanted to see only ingredients. It is important to note that these single word frequencies falsely correlate some ingredients, such as bell pepper and black pepper. In future iterations of this project, the concept of bigrams would be explored to get more accurate analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wordfreq_AggIng_top_wo_extra"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15,10))\n",
    "wordfreq_AggIng_top_wo_extra.sort_values(ascending=False).plot.bar()\n",
    "plt.xticks(rotation=50)\n",
    "plt.title('Word Frequency of Recipe Ingredients (w/o Extra StopWords)')\n",
    "plt.xlabel(\"Ingredient\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It may be interesting to view the above graph of ingredient frequencies as a proportion over the total number of recipes. For example, salt is an ingredient in about 75% of the scraped recipes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wordfreq_Ing_prop.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. USDA Nutrition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analysis tools imported from python script in repository\n",
    "from NS_analysis import *\n",
    "\n",
    "# Analytics by Recipe\n",
    "RecipeAnalyze(analysis=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Analytics by Ingredient\n",
    "IngredientAnalyze(analysis=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Fun Facts\n",
    "Factoids()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Web Scraping"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. USDA Nutrition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Analysis**  \n",
    "Below are boxen-style plots showing the data grouped by recipe and by ingredient. The plots demonstrate the shape of the distribution for each nutrient, as well as its variation.  \n",
    "<br>\n",
    "The plots show that the tendency of the data for each nutrient factor are skewed to the left and closer to 0 than the extremes. In the context of food items and recipes, this makes sense because most foods will have a relatively low average, but occassionally there are foods containing an extreme of a given nutrient. Foods with a low extreme will be closer to the average, since most foods do not have an extreme of any individual nutrient. Also, due to errors in quantity and weight parsing, there are cases in which a food or recipe has exaggerated nutrient values.  \n",
    "<br>\n",
    "Outliers were handled in the _\"Stats and Interpretation\"_ block above. We determined that anything 1.5 interquantile ranges below quantile 0.15 or above quantile 0.85 were outliers, most likely due to parsing errors of quantity and/or weight. When 0.25 and 0.75 were used as quantiles, it removed too many candidates, therefore we adjusted our quantiles to be more inclusive."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualization by Recipe\n",
    "RecipeAnalyze(plot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualization by Ingredient\n",
    "IngredientAnalyze(plot=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
